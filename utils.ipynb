{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import docx2txt\n",
    "import yaml #pip install ppyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soupify(link):\n",
    "    tmp = urllib.request.urlopen(urllib.request.Request(link, headers={'User-Agent': 'Mozilla/5.0'})).read()\n",
    "    soup = BeautifulSoup(tmp, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parser_job_url(soup_0):\n",
    "    url_to_jobpage = ['https://ca.indeed.com/'+ element['href'] for element in soup_0.find_all(name='a', attrs={\"data-tn-element\":\"jobTitle\"})]\n",
    "    return url_to_jobpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_job_detail(link):\n",
    "    \n",
    "    try:\n",
    "        soup_tmp = soupify(link)\n",
    "        '''\n",
    "        jobtitle | company | location | date | description\n",
    "        '''\n",
    "        jobtitle = soup_tmp.find_all(name = \"b\", attrs = {\"class\":\"jobtitle\"})[0].get_text()\n",
    "        \n",
    "        company  = soup_tmp.find_all(name = \"span\", attrs = {\"class\":\"company\"})[0].get_text()\n",
    "        \n",
    "        location = soup_tmp.find_all(name = \"span\", attrs = {\"class\":\"location\"})[0].get_text()\n",
    "        \n",
    "        date = soup_tmp.find_all(name = \"span\", attrs = {\"class\":\"date\"})[0].get_text()\n",
    "        \n",
    "        description_tmp = soup_tmp.find_all(name = 'span', attrs= {\"id\":'job_summary'})[0].get_text()\n",
    "        description_tmp = ' '.join(description_tmp.split())\n",
    "        description = re.sub(\"[-:.!,*+%?//#Â·$;|]\",\" \", description_tmp)\n",
    "        \n",
    "    except:\n",
    "        jobtitle = company = location = date = description = 'invalid'\n",
    "        \n",
    "    return jobtitle, company, location, date, description, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_attr(kw_title, kw_location, kw_province):\n",
    "    url_int = 'https://ca.indeed.com/jobs?q={}&l={}%2C+{}'.format(kw_title, kw_location, kw_province)\n",
    "    url_int = re.sub(' ','+', url_int)\n",
    "    return url_int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_job(soup_0):\n",
    "    \n",
    "    num_jobs = soup_0.find(id='searchCount').get_text().split(' ')[-1]\n",
    "    num_jobs = re.sub(',','',num_jobs)\n",
    "    num_pages = min(int(int(num_jobs) / 10), 50)\n",
    "    \n",
    "    current_time = str(datetime.datetime.now())[0:-7]\n",
    "    \n",
    "    return num_jobs, num_pages, current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(kw_title, kw_location, kw_province):\n",
    "    \n",
    "    url_int = config_attr(kw_title, kw_location, kw_province)\n",
    "    \n",
    "    soup_int = soupify(url_int)\n",
    "\n",
    "    num_jobs, num_pages, current_time = find_num_job(soup_int)\n",
    "\n",
    "    print ('\\n There are {} {} jobs in {} as of {} \\n'.format(num_jobs, kw_title, kw_location, current_time)) \n",
    "    \n",
    "    ctr = 0\n",
    "    \n",
    "    list_final = []\n",
    "\n",
    "    for element in np.arange(num_pages):\n",
    "    \n",
    "        page_ittr = url_int + '&start='+ str(element*10)\n",
    "\n",
    "        print (' \\n Current page is {} \\n '.format(page_ittr))\n",
    "\n",
    "        tmp_0 = soupify(page_ittr)\n",
    "\n",
    "        tmp_1 = parser_job_url(tmp_0)\n",
    "\n",
    "        for element in tmp_1:\n",
    "\n",
    "            tmp_2 = parser_job_detail(element)   \n",
    "            \n",
    "            ctr += 1\n",
    "            \n",
    "            print ('-- Captured {} job postings --'.format(ctr))\n",
    "            \n",
    "            list_final.append(tmp_2)\n",
    "            \n",
    "    return list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_skills_dict():\n",
    "    \n",
    "    '''\n",
    "    load skill-set dictionary, and select the first 200 skillset words/phrases by frequency.\n",
    "\n",
    "    '''\n",
    "    with open(\"config.yml\", 'r') as ymlfile:\n",
    "        cfg = yaml.load(ymlfile)\n",
    "        \n",
    "    df_skill = pd.read_csv(cfg['file_path']['skills_dict'], encoding='latin1', header= None)\n",
    "\n",
    "    print ('\\n Total {} data scientist candidates in this dictionary.'.format(len(df_skill)))\n",
    "\n",
    "    smy_skills = [x for sublist in df_skill.values.tolist() for x in sublist if x != 'None']\n",
    "    smy_skills = pd.Series(smy_skills).apply(lambda x:x.lower())\n",
    "    smy_skills = smy_skills.value_counts()[0:199].index.tolist()\n",
    "    \n",
    "    return smy_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_cosine_similarity(c1, c2):\n",
    "    \n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    smlrt = dotprod / (magA * magB)\n",
    "    print ('The cosine-similarity is {}'.format(smlrt))\n",
    "    \n",
    "    return smlrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main('data science','toronto','on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
